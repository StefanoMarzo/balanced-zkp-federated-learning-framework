{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d924c2af",
   "metadata": {},
   "source": [
    "### Imports and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69f538d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, recall_score, roc_curve, multilabel_confusion_matrix, confusion_matrix\n",
    "from aif360.sklearn.metrics import equal_opportunity_difference\n",
    "\n",
    "#Metrics\n",
    "from metrics import *\n",
    "#Utils\n",
    "from utils import *\n",
    "#Properties\n",
    "from utils import properties as p\n",
    "\n",
    "\n",
    "#Model\n",
    "import nn_model as nn\n",
    "\n",
    "#Config\n",
    "%config InlineBackend.figure_format='retina'\n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a0ae0f",
   "metadata": {},
   "source": [
    "### Setup properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53bdf63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.update({\n",
    "    \n",
    "    #Age range\n",
    "    'age_min' : 0,\n",
    "    'age_max' : 39,\n",
    "    \n",
    "    #Number of data samples\n",
    "    'data_samples' : 20000,\n",
    "    'privileged_group_proportion': 0.85,\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4791142",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17e129e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset/data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b234f7",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "923e8783",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove other ethnicities\n",
    "del p['ETHNICITIES'][4]\n",
    "df = df[df['ethnicity'] != 4]\n",
    "#print_summary(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "481b5ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_filter(df, age_min, age_max):\n",
    "    return (df['age'] >= age_min) & (df['age'] <= age_max)\n",
    "\n",
    "def ethnicity_gender_filter(df, i, j):\n",
    "    return (df['ethnicity'] == p['ETHNICITIES'][i]) & (df['gender'] == p['GENDERS'][j])\n",
    "\n",
    "def ethnicity_age_filter(df, e, age_min, age_max):\n",
    "    return (df['ethnicity'] == p['ETHNICITIES'][e]) & age_filter(df, age_min, age_max)\n",
    "\n",
    "def ethnicity_gender_age_filter(df, i, j, age_min, age_max):\n",
    "    return ethnicity_gender_filter(df, i, j) & age_filter(df, age_min, age_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "687bcfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare\n",
    "df = df.loc[age_filter(df, p['age_min'], p['age_max'])]\n",
    "df['ethnicity'] = df['ethnicity'].map(p['ETHNICITIES'])\n",
    "df['gender'] = df['gender'].map(p['GENDERS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740987e5",
   "metadata": {},
   "source": [
    "### Create balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64e807db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ethnicity gender age sample size\n",
    "ethnicity_age_sample_size = round(p['data_samples'] / (round(p['age_range']() / p['age_bins']) * len(p['ETHNICITIES'])))\n",
    "\n",
    "balanced_df = pd.concat([\n",
    "    df.loc[ethnicity_age_filter(df, e, age_m, age_M)]\\\n",
    "        .sample(ethnicity_age_sample_size, random_state=p['seed'], replace=True)\n",
    "    for e in range(len(p['ETHNICITIES']))\n",
    "    #for j in range(len(GENDERS))\n",
    "    for (age_m, age_M) in p['bins']()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f40c0a3",
   "metadata": {},
   "source": [
    "### Create unbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51b6ee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unbalanced_set(df, minority_proportion=0.05, majority_group=0):\n",
    "    min_factor = minority_proportion # 5% of the population \n",
    "    maj_factor = 1 - min_factor\n",
    "\n",
    "    #Size of minority group\n",
    "    min_sample_size = round(p['data_samples'] * min_factor / len(p['bins']()) / (len(p['ETHNICITIES']) - 1))\n",
    "\n",
    "    #Size of majority group\n",
    "    maj_sample_size = round(p['data_samples'] * maj_factor / len(p['GENDERS']) / len(p['bins']()))\n",
    "\n",
    "    maj_male = pd.concat([\n",
    "        df.loc[ethnicity_gender_age_filter(df, majority_group, 0, age_min, age_max)]\\\n",
    "            .sample(maj_sample_size, random_state=p['seed'], replace=True)\n",
    "        for (age_min, age_max) in p['bins']()\n",
    "    ])\n",
    "\n",
    "    maj_female = pd.concat([\n",
    "        df.loc[ethnicity_gender_age_filter(df, majority_group, 1, age_min, age_max)]\\\n",
    "            .sample(maj_sample_size, random_state=p['seed'], replace=True)\n",
    "        for (age_min, age_max) in p['bins']()\n",
    "    ])\n",
    "\n",
    "    unbalanced_df = pd.concat([\n",
    "        df.loc[ethnicity_age_filter(df, e, age_min, age_max)]\\\n",
    "            .sample(min_sample_size, random_state=p['seed'], replace=True)\n",
    "        for e in range(len(p['ETHNICITIES']))\n",
    "        #for j in range(len(GENDERS))\n",
    "        for (age_min, age_max) in p['bins']()\n",
    "        if e != majority_group\n",
    "    ] + [maj_female, maj_male])\n",
    "    \n",
    "    return unbalanced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a907e00f",
   "metadata": {},
   "source": [
    "## Predictive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b678f05",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a06b5d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "epochs_n = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ccb695",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdbd2dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# network (not federated)\n",
    "\n",
    "def standard_nn():\n",
    "    return nn.get_standard_nn(p['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852aa5ed",
   "metadata": {},
   "source": [
    "### Evaluate predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98b1a550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ethnicity(dataset, ethnicity, df_test, model):\n",
    "    ethn_y, ethn_y_pred = get_ethnicity_predictions(dataset, df_test, ethnicity, model)\n",
    "    score = f1_score(convert_output(ethn_y), convert_output(ethn_y_pred), average='macro')\n",
    "    return score\n",
    "\n",
    "def evaluate_all_ethnicities(dataset, df_test, model):\n",
    "    scores = [evaluate_ethnicity(dataset, e, df_test, model) for e in p['ETHNICITIES'].values()]\n",
    "    return dict(zip(p['ETHNICITIES'].values(), scores))\n",
    "\n",
    "def get_ethnicity_predictions(dataset, df_test, ethnicity, model):\n",
    "    idx_keys = dataset.index.intersection(df_test[df_test['ethnicity'] == ethnicity].index)\n",
    "    ethn_df = dataset.loc[idx_keys]\n",
    "    ethn_X = prepare_X(ethn_df)\n",
    "    ethn_y = prepare_y(ethn_df)         #y_true \n",
    "    ethn_y_pred = model.predict(ethn_X) #y_pred \n",
    "    return ethn_y, ethn_y_pred\n",
    "\n",
    "def ethnicity_equal_odd_difference(dataset, df_test, ethnicity, model, privileged_group, ethn_y_p, ethn_y_p_pred):\n",
    "    \n",
    "    if ethnicity == privileged_group:\n",
    "        return 0\n",
    "    \n",
    "    ethn_y_u, ethn_y_u_pred = get_ethnicity_predictions(dataset, df_test, ethnicity, model) #unprivileged\n",
    "    \n",
    "    return np.mean(equal_odd_difference(\n",
    "        one_hot_vect_to_class(convert_output(ethn_y_p)), \n",
    "        one_hot_vect_to_class(convert_output(ethn_y_p_pred)), \n",
    "        one_hot_vect_to_class(convert_output(ethn_y_u)), \n",
    "        one_hot_vect_to_class(convert_output(ethn_y_u_pred))\n",
    "    ))\n",
    "\n",
    "def ethnicity_equality_metric(\n",
    "    dataset, \n",
    "    df_test, \n",
    "    ethnicity, \n",
    "    model, \n",
    "    privileged_group, \n",
    "    ethn_y_p, \n",
    "    ethn_y_p_pred, \n",
    "    metric_fun):\n",
    "    if ethnicity == privileged_group:\n",
    "        return 0\n",
    "    \n",
    "    ethn_y_u, ethn_y_u_pred = get_ethnicity_predictions(dataset, df_test, ethnicity, model) #unprivileged\n",
    "    \n",
    "    return np.nanmean(metric_fun(\n",
    "        one_hot_vect_to_class(convert_output(ethn_y_p)), \n",
    "        one_hot_vect_to_class(convert_output(ethn_y_p_pred)), \n",
    "        one_hot_vect_to_class(convert_output(ethn_y_u)), \n",
    "        one_hot_vect_to_class(convert_output(ethn_y_u_pred))\n",
    "    ))\n",
    "\n",
    "def all_ethnicity_equal_difference(\n",
    "    dataset, df_test, model, privileged_group, ethn_y_p, ethn_y_p_pred, metric_fn_list\n",
    "):\n",
    "    scores = [\n",
    "        np.nanmean([\n",
    "            ethnicity_equality_metric(\n",
    "                dataset, \n",
    "                df_test, \n",
    "                e, \n",
    "                model, \n",
    "                privileged_group, \n",
    "                ethn_y_p, \n",
    "                ethn_y_p_pred,\n",
    "                metric_fn\n",
    "            ) \n",
    "            for e in p['ETHNICITIES'].values()\n",
    "        ]) for metric_fn in metric_fn_list\n",
    "    ]\n",
    "    metrics_list = ['Avg. ' + m.__name__.replace('_', ' ').capitalize() for m in metric_fn_list]\n",
    "    return dict(zip(metrics_list, scores)) # + scores_eodd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8599b8",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "#### Params \n",
    "`macro`: Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account. <br><br>\n",
    "`weighted`: Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52b8350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_evaluate(df, model, exp_id):\n",
    "    \n",
    "    privileged_group = df['ethnicity'].value_counts().idxmax() #Majority group (privileged)\n",
    "    \n",
    "    df_train, df_test = create_train_test(df)\n",
    "    X_train, y_train, X_test, y_test = prepare_data(df_train, df_test)\n",
    "    for i in range(len(X_train)):\n",
    "        model.fit(X_train[i], y_train[i], epochs=epochs_n, learning_rate=learning_rate)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    ethn_y_p, ethn_y_p_pred = get_ethnicity_predictions(df, df_test, privileged_group, model) #privileged\n",
    "    \n",
    "    all_ethnicities_f1s = evaluate_all_ethnicities(df, df_test, model)\n",
    "\n",
    "    \n",
    "    ev = dict()\n",
    "    ev['experiment ID'] = exp_id\n",
    "    ev['data sample #'] = round(len(df))\n",
    "    ev['f1'] = f1_score(convert_output(y_test), convert_output(y_pred), average='macro')\n",
    "    ev['f1 variance'] = macro_variance(ev['f1'], list(all_ethnicities_f1s.values()))\n",
    "    #ev.update(all_ethnicities_f1s)\n",
    "    all_parities = all_ethnicity_equal_difference(\n",
    "        df, df_test, model, privileged_group, ethn_y_p, ethn_y_p_pred,\n",
    "        [\n",
    "            disparate_impact_ratio, \n",
    "            statistical_parity_difference, \n",
    "            equal_opportunity_difference,\n",
    "            equal_odd_difference\n",
    "        ]\n",
    "    )\n",
    "    ev.update(all_parities)\n",
    "    #ev['EOR'] = equal_opportunity_difference(convert_output(y_test), convert_output(y_pred))\n",
    "    print_summary(df, exp_id)\n",
    "    \n",
    "    return ev\n",
    "\n",
    "def compare_model_behaviour(df_list, model_list, exp_ids):\n",
    "    return [predict_and_evaluate(df_list[i], model_list[i], exp_ids[i]) for i in range(len(df_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70317431",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [\n",
    "    df.sample(p['data_samples'], random_state=p['seed'], replace=True), #Sample of the actual set\n",
    "    balanced_df                                                         #Artificial Balanced set\n",
    "] + [\n",
    "    create_unbalanced_set(df, 0.05, i)                                   #All minority groups set\n",
    "    for i in p['ETHNICITIES'].keys()\n",
    "]\n",
    "\n",
    "models = [standard_nn()] * len(dfs)\n",
    "\n",
    "exp_ids = ['Dataset', 'Balanced dataset'] + [\n",
    "    p['ETHNICITIES'][i] + ' Majority unbalanced dataset'\n",
    "    for i in p['ETHNICITIES'].keys()\n",
    "]\n",
    "\n",
    "metrics = compare_model_behaviour(\n",
    "    dfs, \n",
    "    models,\n",
    "    exp_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e1ab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%notify\n",
    "\n",
    "def get_metrics_data(metrics):\n",
    "    metrics_df = pd.DataFrame()\n",
    "    for m in metrics:\n",
    "        row = pd.Series(m)\n",
    "        metrics_df = metrics_df.append(row, ignore_index=True)\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023cac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics_data(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca2e52f",
   "metadata": {},
   "source": [
    "### Human evaluation & plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c542b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_size = 10\n",
    "s_bal_df = balanced_df.reset_index().sample(s_size).reset_index()\n",
    "for idx in range(len(s_bal_df)):\n",
    "    predict_row(s_bal_df, idx, models[1])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
